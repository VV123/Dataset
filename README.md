# LLM

This is a survey of Large Language Models (LLM).

---------------------------------------

## Papers

* [Finetuned Language Models Are Zero-Shot Learners](https://pile.eleuther.ai/)
* [SimVLM: Simple Visual Language Model Pretraining with Weak Supervision] (https://arxiv.org/abs/2108.10904)
* [Gpt-neox-20b: An open-source autoregressive language model] (https://arxiv.org/abs/2204.06745)
* [Scaling Language Models: Methods, Analysis & Insights from Training Gopher] (https://arxiv.org/abs/2112.11446)
* [Training language models to follow instructions with human feedback] (https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html)
* [Domain-specific language model pretraining for biomedical natural language processing] (https://dl.acm.org/doi/abs/10.1145/3458754?casa_token=P-T8trc32d8AAAAA:94zBcf_gj0Ht5jLClGczKrM22PkBDJGvBHtYgI3P76BJHqz8OnfZsi8d7XAyfV4Nm0YbQsXtKFrf)
* [Flamingo: a Visual Language Model for Few-Shot Learning] (https://arxiv.org/abs/2205.01068)
* [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, a Large-Scale Generative Language Model] (https://arxiv.org/abs/2201.11990)




  Paper - Training Compute-Optimal Large Language Models
  


## Dataset
* Data 1 (The paper uses a number of different datasets for their experiments. The main focus of the paper is on investigating the zero-shot capabilities of finetuned language models, and the authors use a variety of benchmark datasets to evaluate the performance of their models.Here is a brief overview of the datasets used in the paper:
GLUE: The General Language Understanding Evaluation (GLUE) benchmark consists of a collection of nine different natural language understanding tasks, including sentiment analysis, natural language inference, and question answering. The authors use this dataset to evaluate the performance of their models on a range of different tasks.
SuperGLUE: The SuperGLUE benchmark is an extension of the GLUE benchmark, consisting of a set of eight more difficult natural language understanding tasks. The authors use this dataset to evaluate the performance of their models on more challenging tasks.
SQuAD: The Stanford Question Answering Dataset (SQuAD) consists of a set of Wikipedia articles and associated questions, where the goal is to answer the questions based on the information in the articles. The authors use this dataset to evaluate the performance of their models on question answering tasks.
TriviaQA: The TriviaQA dataset is another question answering dataset, but with a focus on more difficult questions that require a deeper understanding of the text. The authors use this dataset to evaluate the performance of their models on more challenging question answering tasks.
OpenAI Codex: The OpenAI Codex dataset consists of a large collection of code snippets and natural language descriptions of what the code does. The authors use this dataset to evaluate the performance of their models on code generation tasks.)

* Data 2 (The authors use the Conceptual Captions dataset, is a large-scale image captioning dataset, which consists of over 3.3 million images with corresponding captions. The images cover a wide range of topics and come from a variety of sources, including Flickr and Google Images. The captions are generated by annotators, who are asked to describe the content of the image in a natural language sentence.
For their experiments, the authors use a preprocessed version of the Conceptual Captions dataset, which includes only the images and captions that contain at least one noun and one verb. The resulting dataset contains around 2.8 million image-caption pairs.
The authors use this dataset to pretrain their visual language model using a weakly supervised approach, where the model is trained to predict the next word in the caption given the previous words and the corresponding image. The authors show that their approach outperforms existing methods on a range of downstream tasks, including image captioning and visual question answering.)

* Data 3 (The paper presents GPT-NeoX-20B, an open-source autoregressive language model with 20 billion parameters. The authors used a variety of datasets to train and evaluate the model, including:
Common Crawl: The Common Crawl dataset is a large collection of web pages that have been crawled and archived. The authors used the Common Crawl dataset to pretrain the GPT-NeoX-20B model.
Pile: Pile is a large, diverse, and open-sourced dataset created by concatenating over 800 public datasets. The Pile dataset includes a wide range of text, including books, scientific articles, news articles, and web pages. The authors used the Pile dataset to fine-tune the GPT-NeoX-20B model.
SuperGLUE: The SuperGLUE benchmark is an extension of the GLUE benchmark, consisting of a set of eight more difficult natural language understanding tasks. The authors used the SuperGLUE benchmark to evaluate the performance of the GPT-NeoX-20B model on a range of different tasks.
WMT14: The WMT14 dataset is a collection of parallel corpora for machine translation, consisting of English and German text. The authors used the WMT14 dataset to evaluate the performance of the GPT-NeoX-20B model on machine translation tasks.
LAMBADA: The LAMBADA dataset is a language modeling benchmark where the goal is to predict the final word of a sentence given the preceding words. The authors used the LAMBADA dataset to evaluate the GPT-NeoX-20B model's ability to handle long-term dependencies.
In addition to these datasets, the authors also used a number of other datasets for specific experiments, such as the GYAFC dataset for evaluating the model's ability to generate coherent text and the COCO dataset for image captioning tasks.)

* Data 4 (The authors used a variety of datasets to train and fine-tune the Gopher language model, with a particular emphasis on web text and biomedical literature.Datasets used for pretraining and fine-tuning the Gopher language model, includes:
Common Crawl: Common Crawl is a large dataset of web pages that have been crawled and archived. The authors used Common Crawl as a primary data source for pretraining Gopher.
PubMed: PubMed is a large biomedical literature database maintained by the US National Library of Medicine. The authors used PubMed to create a biomedical language model.
Wikipedias: The authors used Wikipedias in multiple languages to train Gopher language models for multiple languages.
OpenWebText: OpenWebText is a large dataset of web pages, similar to Common Crawl, that has been specifically curated for use in language modeling. The authors used OpenWebText for both pretraining and fine-tuning Gopher.
Reddit: Reddit is a social media platform with a large amount of user-generated text. The authors used Reddit to fine-tune Gopher on conversational and informal text.)

* COIN [Github]() [Paper]() (The authors created a new dataset called COIN, which stands for "Collaborative Instruction and Navigation." COIN dataset provides a novel task of training language models to follow natural language instructions in complex environments, and enables the evaluation of language models' ability to understand and navigate in such environments based on natural language instructions..The COIN dataset consists of 13,003 natural language instructions paired with 133,995 navigation environments. The navigation environments are 2D maps with objects that the language models must navigate to in order to follow the instructions correctly. The objects in the environment are categorized into three types: targets (objects that the language model should navigate to), distractors (objects that should be avoided), and neutral objects (objects that are not relevant to the instruction).
To create the COIN dataset, the authors used a 3D game engine to generate 2D navigation environments with objects, and then collected natural language instructions from human annotators through a crowdsourcing platform. The authors also collected human feedback on the performance of language models trained on COIN, which they used to improve the models' ability to follow natural language instructions.)

* Data 6 [Paper]() (The authors of the paper do not mention any specific dataset used for their experiments. However, they do mention that they evaluate their method on several benchmark datasets in biomedical NLP, including the i2b2 2010 challenge dataset, the SemEval-2014 Task 7 dataset, and the ShARe/CLEF eHealth Evaluation Lab 2014 dataset. These datasets contain a variety of tasks related to clinical text analysis, such as named entity recognition, relation extraction, and classification.)

* Data 7 [Paper]() (The paper proposes a new visual language model called Flamingo, which can learn from just a few examples and achieve state-of-the-art performance on few-shot learning benchmarks.The authors do not use a single specific dataset in the paper, but instead evaluate Flamingo's performance on a variety of few-shot learning benchmarks, including:
Omniglot: a handwritten character recognition dataset, which contains 1623 different handwritten characters from 50 different alphabets.
Mini-ImageNet: a subset of the ImageNet dataset, which contains 100 classes with 600 images each.
FewRel: a few-shot relation extraction dataset, which contains sentences and corresponding entity pairs, and the goal is to predict the relation between them.
Text-to-SQL: a dataset for translating natural language questions into SQL queries, which contains a small number of annotated examples for each query type.
The authors also perform an ablation study on the different components of Flamingo and evaluate its performance on other few-shot learning tasks, including visual question answering (VQA), few-shot slot filling, and few-shot dialog state tracking.)

* Data 8 (The paper describes the training of Megatron-Turing NLG 530B, a very large-scale generative language model.The paper does not describe the specific dataset used to train the model. However, it mentions that the authors used a diverse set of text corpora from various sources to pretrain the model, including web pages, news articles, books, and scientific papers. The total size of the corpus is not mentioned, but it is described as being "multi-terabyte."
The authors preprocessed the data using a set of standard techniques, including tokenization, subword segmentation, and data shuffling. They also applied data augmentation techniques to increase the diversity of the training data.
To train the model, the authors used the DeepSpeed and Megatron libraries, which are optimized for training large-scale language models on distributed computing clusters. They used a combination of data parallelism and model parallelism to distribute the training across multiple GPUs.
The paper also describes the performance of the trained model on several language modeling benchmarks, including the LAMBADA and WikiText-103 datasets. The authors report state-of-the-art performance on both benchmarks.)

* Data [Github](https://github.com/kojima-takeshi188/zero_shot_cot) ( Experiments used publicly available datasets except for “Last
Letters” and “Coin Flip” datasets. We created these two datasets.
(*1) N : Number, M : Pick up one from multiple choices, Y : Answer Yes or No, F : Free Format.
(*2) Average number of words in questions texts.) 
 
* Data [https://github.com/openai/code-align-evals-data] ( Training dataset was collected in May 2020 from 54 million public software repositories hosted on GitHub, containing 179 GB of unique Python files under 1 MB. We filtered
out files which were likely auto-generated, had average line
length greater than 100, had maximum line length greater
than 1000, or contained a small percentage of alphanumeric
characters. After filtering, our final dataset totaled 159 GB.)



* [PILE](https://pile.eleuther.ai/) ~800G text

## Contact

Wenlu Wang
wenlu.wang.1@gmail.com
