# LLM

This is a survey of Large Language Models (LLM).

---------------------------------------

## Papers

* [paper title]()

## Dataset

* data1 [https://github.com/kojima-takeshi188/zero_shot_cot]( Experiments used publicly available datasets except for “Last
Letters” and “Coin Flip” datasets. We created these two datasets.
(*1) N : Number, M : Pick up one from multiple choices, Y : Answer Yes or No, F : Free Format.
(*2) Average number of words in questions texts.) 
 
* data2 [https://github.com/openai/code-align-evals-data] ( Training dataset was collected in May 2020 from 54 million public software repositories hosted on GitHub, containing 179 GB of unique Python files under 1 MB. We filtered
out files which were likely auto-generated, had average line
length greater than 100, had maximum line length greater
than 1000, or contained a small percentage of alphanumeric
characters. After filtering, our final dataset totaled 159 GB.)



* [PILE](https://pile.eleuther.ai/) ~800G text

## Contact

Wenlu Wang
wenlu.wang.1@gmail.com
